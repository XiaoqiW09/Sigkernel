{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiaoqiW09/Sigkernel/blob/main/sigkernel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqlmsmi3aEE4",
        "outputId": "578e9836-22bb-4346-d50b-d72319b6019b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/crispitagorico/sigkernel.git\n",
            "  Cloning https://github.com/crispitagorico/sigkernel.git to /tmp/pip-req-build-7qvt8_te\n",
            "  Running command git clone -q https://github.com/crispitagorico/sigkernel.git /tmp/pip-req-build-7qvt8_te\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (0.29.32)\n",
            "Requirement already satisfied: numba>=0.50 in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (0.56.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (1.12.1+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (1.0.2)\n",
            "Requirement already satisfied: tslearn in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (0.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (1.7.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.50->sigkernel==0.0.1) (0.39.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.7/dist-packages (from numba>=0.50->sigkernel==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.50->sigkernel==0.0.1) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.50->sigkernel==0.0.1) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sigkernel==0.0.1) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.50->sigkernel==0.0.1) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sigkernel==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sigkernel==0.0.1) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/crispitagorico/sigkernel.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6t6T70nvSge",
        "outputId": "0c887cc0-f045-40b7-cb58-2beb2bc6299b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting signatory\n",
            "  Downloading signatory-1.2.6.1.9.0.tar.gz (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: signatory\n",
            "  Building wheel for signatory (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for signatory: filename=signatory-1.2.6.1.9.0-cp37-cp37m-linux_x86_64.whl size=7554077 sha256=e34b871a89cc5eb1d79cdc0b7fb52008900af90c95f43facf98919df17e0c04b\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/ff/e5/ffe613433c810f7f82da6e0c55abd15f4cc04960f8137db53b\n",
            "Successfully built signatory\n",
            "Installing collected packages: signatory\n",
            "Successfully installed signatory-1.2.6.1.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install signatory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTGNlnuCwBYX",
        "outputId": "48a25220-e829-4ff0-e56f-a64feeb2f425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mne\n",
            "  Downloading mne-1.2.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 19.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-1.2.1\n"
          ]
        }
      ],
      "source": [
        "pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_JYTtKG1J3v",
        "outputId": "7b9e6b67-f87a-41a5-b9f3-25bd9a7ad56d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting iisignature\n",
            "  Downloading iisignature-0.24.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>1.7 in /usr/local/lib/python3.7/dist-packages (from iisignature) (1.21.6)\n",
            "Building wheels for collected packages: iisignature\n",
            "  Building wheel for iisignature (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iisignature: filename=iisignature-0.24-cp37-cp37m-linux_x86_64.whl size=2528292 sha256=640205f1b9bbb86dcdeed888f9e7b2fa5415de7ee6baf24b91fc9d81cc2a288a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/99/a3/b2b3f95df7c69f664e92b4ec58b8da922cb95bc394700cbc0f\n",
            "Successfully built iisignature\n",
            "Installing collected packages: iisignature\n",
            "Successfully installed iisignature-0.24\n"
          ]
        }
      ],
      "source": [
        "pip install iisignature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CszZk9_YuSVz"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import signatory\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sigkernel\n",
        "import torch \n",
        "import math\n",
        "import pickle\n",
        "from time import sleep\n",
        "import iisignature as sig\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
        "from tslearn.datasets import UCR_UEA_datasets\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance,TimeSeriesScalerMinMax\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tslearn.svm import TimeSeriesSVC\n",
        "from sklearn.svm import SVC\n",
        "from tslearn.datasets import UCR_UEA_datasets\n",
        "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
        "from tslearn.svm import TimeSeriesSVC\n",
        "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "import joblib\n",
        "import h5py\n",
        "import mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N09eHLWkwHyF",
        "outputId": "f94520ef-c186-4614-bc2b-050f17c3a63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YZfia25N-8k4"
      },
      "outputs": [],
      "source": [
        "data_list = ['alcoholic_1', 'alcoholic_12', 'alcoholic_21']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mb_Mz3a4wAyt"
      },
      "outputs": [],
      "source": [
        "def write_alcoholic(dataset='alcoholic_1'):\n",
        "    # S1: S1 obj - a single object shown;\n",
        "    s1 = 0\n",
        "    # S12: S2 nomatch - object 2 shown in a non-matching condition (S1 differed from S2)\n",
        "    s12 = 0\n",
        "    # S21: S2 match - object 2 shown in a matching condition (S1 was identical to S2),\n",
        "    s21 = 0\n",
        "    # initialise numpy arrays to fill with time series\n",
        "    s1_X_train_unscaled = np.zeros((160, 256, 64))\n",
        "    s1_y_train = np.zeros(160)\n",
        "    s21_X_train_unscaled = np.zeros((159, 256, 64))\n",
        "    s21_y_train = np.zeros(159)\n",
        "    s12_X_train_unscaled = np.zeros((149, 256, 64))\n",
        "    s12_y_train = np.zeros(149)\n",
        "\n",
        "    # assign numerical values to the classes\n",
        "    classifier = {'a': 1, 'c': 0}\n",
        "\n",
        "    # run through every file in the train directory and import it, using information from the\n",
        "    # matching condition column to determine which experiment was being conducted.\n",
        "    # using this, put the data into the corresponding numpy array\n",
        "    filenames_list = os.listdir('/content/drive/My Drive/alcohol train data')\n",
        "\n",
        "    for file_name in tqdm(filenames_list):\n",
        "        temp_df = pd.read_csv('/content/drive/My Drive/alcohol train data/' + file_name)\n",
        "        if temp_df[\"matching condition\"][0] == \"S1 obj\":\n",
        "            s1_X_train_unscaled[s1] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "            s1_y_train[s1] = classifier[temp_df['subject identifier'][0]]\n",
        "            s1 += 1\n",
        "        if temp_df[\"matching condition\"][0] == \"S2 match\":\n",
        "            s21_X_train_unscaled[s21] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "            s21_y_train[s21] = classifier[temp_df['subject identifier'][0]]\n",
        "            s21 += 1\n",
        "        if temp_df[\"matching condition\"][0] == \"S2 nomatch,\":\n",
        "            s12_X_train_unscaled[s12] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "            s12_y_train[s12] = classifier[temp_df['subject identifier'][0]]\n",
        "            s12 += 1\n",
        "\n",
        "    # t1: S1 obj - a single object shown;\n",
        "    t1 = 0\n",
        "    # t12: S2 nomatch - object 2 shown in a non-matching condition (S1 differed from S2)\n",
        "    t12 = 0\n",
        "    # t21: S2 match - object 2 shown in a matching condition (S1 was identical to S2),\n",
        "    t21 = 0\n",
        "    t1_X_test_unscaled = np.zeros((160, 256, 64))\n",
        "    t1_y_test = np.zeros(160)\n",
        "    t21_X_test_unscaled = np.zeros((160, 256, 64))\n",
        "    t21_y_test = np.zeros(160)\n",
        "    t12_X_test_unscaled = np.zeros((160, 256, 64))\n",
        "    t12_y_test = np.zeros(160)\n",
        "\n",
        "    # same as above but for test data\n",
        "    classifier = {'a': 1, 'c': 0}\n",
        "\n",
        "    # list of filenames in the directory\n",
        "    filenames_list = os.listdir('/content/drive/My Drive/UROP alcohol test data')\n",
        "\n",
        "    for file_name in tqdm(filenames_list):\n",
        "        if file_name == \"Test\":\n",
        "            pass\n",
        "        else:\n",
        "            temp_df = pd.read_csv('/content/drive/My Drive/UROP alcohol test data/' + file_name)\n",
        "            if temp_df[\"matching condition\"][0] == \"S1 obj\":\n",
        "                t1_X_test_unscaled[t1] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "                t1_y_test[t1] = classifier[temp_df['subject identifier'][0]]\n",
        "                t1 += 1\n",
        "            if temp_df[\"matching condition\"][0] == \"S2 match\":\n",
        "                t21_X_test_unscaled[t21] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "                t21_y_test[t21] = classifier[temp_df['subject identifier'][0]]\n",
        "                t21 += 1\n",
        "            if temp_df[\"matching condition\"][0] == \"S2 nomatch,\":\n",
        "                t12_X_test_unscaled[t12] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "                t12_y_test[t12] = classifier[temp_df['subject identifier'][0]]\n",
        "                t12 += 1\n",
        "\n",
        "    if dataset == 'alcoholic_1':\n",
        "        X_train = s1_X_train_unscaled\n",
        "        y_train = s1_y_train\n",
        "        X_test = t1_X_test_unscaled\n",
        "        y_test = t1_y_test\n",
        "    elif dataset == 'alcoholic_12':\n",
        "        X_train = s12_X_train_unscaled\n",
        "        y_train = s12_y_train\n",
        "        X_test = t12_X_test_unscaled\n",
        "        y_test = t12_y_test\n",
        "    elif dataset == 'alcoholic_21':\n",
        "        X_train = s21_X_train_unscaled\n",
        "        y_train = s21_y_train\n",
        "        X_test = t21_X_test_unscaled\n",
        "        y_test = t21_y_test\n",
        "    else:\n",
        "        X_train = None\n",
        "        y_train = None\n",
        "        X_test = None\n",
        "        y_test = None\n",
        "\n",
        "    # f = h5py.File(\"/content/drive/My Drive/data/data.h5\", 'a')\n",
        "    # grp = f.create_group(f\"alcoholic_{subset}\")\n",
        "    # grp.create_dataset(\"X_train\", data=X_train, compression=\"gzip\", compression_opts=7)\n",
        "    # grp.create_dataset(\"y_train\", data=y_train, compression=\"gzip\", compression_opts=7)\n",
        "    # grp.create_dataset(\"X_test\", data=X_test, compression=\"gzip\", compression_opts=7)\n",
        "    # grp.create_dataset(\"y_test\", data=y_test, compression=\"gzip\", compression_opts=7)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmsxwvJ7yPWs"
      },
      "outputs": [],
      "source": [
        "def move_to_hdf():\n",
        "    for i in tqdm(os.listdir(\"/content/drive/My Drive/data\")):\n",
        "        if i.endswith(\"npz\"):\n",
        "            npz = np.load(f\"data/{i}\", allow_pickle=True)\n",
        "            X_train = npz['arr_0']\n",
        "            y_train = npz['arr_1']\n",
        "            X_test = npz['arr_2']\n",
        "            y_test = npz['arr_3']\n",
        "\n",
        "            print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep=\"\\n\")\n",
        "\n",
        "            f = h5py.File(\"/content/drive/My Drive/data/data.h5\", 'a')\n",
        "            grp = f.create_group(i[:-4])\n",
        "            grp.create_dataset(\"X_train\", data=X_train, compression=\"gzip\", compression_opts=7)\n",
        "            grp.create_dataset(\"y_train\", data=y_train, compression=\"gzip\", compression_opts=7)\n",
        "            grp.create_dataset(\"X_test\", data=X_test, compression=\"gzip\", compression_opts=7)\n",
        "            grp.create_dataset(\"y_test\", data=y_test, compression=\"gzip\", compression_opts=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVALZJQmK7e1"
      },
      "outputs": [],
      "source": [
        "def read_data(dataset=\"alcoholic_1\"):\n",
        "    data = h5py.File(\"/content/drive/My Drive/data/data.h5\", 'r')\n",
        "    X_train = data[f\"{dataset}/X_train\"][:]\n",
        "    y_train = data[f\"{dataset}/y_train\"][:]\n",
        "    X_test = data[f\"{dataset}/X_test\"][:]\n",
        "    y_test = data[f\"{dataset}/y_test\"][:]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS9OOKE8zAQK"
      },
      "outputs": [],
      "source": [
        "### ml methods ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KMAoGPWzEto"
      },
      "outputs": [],
      "source": [
        "def ml_method_setup(method, X_train, sig_train, y_train, file_name, reduced=False):\n",
        "    if method == 'ts_knn':\n",
        "        \n",
        "        \n",
        "            if reduced:\n",
        "                clf = GridSearchCV(\n",
        "                    Pipeline([\n",
        "                        ('knn', KNeighborsTimeSeriesClassifier())\n",
        "                    ]),\n",
        "                    {'knn__n_neighbors': range(3, 30, 6), 'knn__weights': ['uniform', 'distance']},\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0),\n",
        "                    n_jobs=-1,\n",
        "                    verbose=10\n",
        "                )\n",
        "            else:\n",
        "                clf = GridSearchCV(\n",
        "                    Pipeline([\n",
        "                        ('knn', KNeighborsTimeSeriesClassifier())\n",
        "                    ]),\n",
        "                    {'knn__n_neighbors': range(3, 30, 2), 'knn__weights': ['uniform', 'distance']},\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0),\n",
        "                    n_jobs=-1,\n",
        "                    verbose=10\n",
        "                )\n",
        "            clf.fit(X_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'ts_svc':\n",
        "      \n",
        "        \n",
        "            if reduced:\n",
        "                clf = GridSearchCV(\n",
        "                    Pipeline([\n",
        "                        ('svc', TimeSeriesSVC(random_state=0, probability=True))\n",
        "                    ]),\n",
        "                    {'svc__kernel': ['rbf', 'poly'], 'svc__shrinking': [True, False],\n",
        "                     'svc__C': [0.1, 1, 10]},\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0),\n",
        "                    n_jobs=-1,\n",
        "                    verbose=10\n",
        "                )\n",
        "            else:\n",
        "                clf = GridSearchCV(\n",
        "                    Pipeline([\n",
        "                        ('svc', TimeSeriesSVC(random_state=0, probability=True))\n",
        "                    ]),\n",
        "                    {'svc__kernel': ['gak', 'rbf', 'poly'], 'svc__shrinking': [True, False],\n",
        "                     'svc__C': [0.1, 0.2, 0.5, 1, 2, 5, 10]},\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0),\n",
        "                    n_jobs=-1,\n",
        "                    verbose=10\n",
        "                )\n",
        "            clf.fit(X_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'lr':\n",
        "        \n",
        "       \n",
        "            lr = LogisticRegression(random_state=0)\n",
        "            parameters = {'C': [0.1, 0.2, 0.5, 1, 2, 5, 10],\n",
        "                          'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
        "            clf = GridSearchCV(lr, parameters, n_jobs=-1, verbose=10)\n",
        "            clf.fit(sig_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'svc':\n",
        "        \n",
        "            svc = SVC(random_state=0, probability=True)\n",
        "            if reduced:\n",
        "                parameters = {'kernel': ['rbf', 'poly'], 'shrinking': [True, False],\n",
        "                              'C': [0.1, 1, 10]}\n",
        "            else:\n",
        "                parameters = {'kernel': ['rbf', 'poly'], 'shrinking': [True, False],\n",
        "                              'C': [0.1, 0.2, 0.5, 1, 2, 5, 10]}\n",
        "            clf = GridSearchCV(svc, parameters, n_jobs=-1, verbose=10)\n",
        "            clf.fit(sig_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'knn':\n",
        "        \n",
        "            knn = KNeighborsClassifier()\n",
        "            parameters = {'n_neighbors': range(3, 30, 2), 'weights': ['uniform', 'distance']}\n",
        "            clf = GridSearchCV(knn, parameters, n_jobs=-1, verbose=10)\n",
        "            clf.fit(sig_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'ada':\n",
        "        \n",
        "            ada = AdaBoostClassifier(random_state=0)\n",
        "            if reduced:\n",
        "                parameters = {'n_estimators': [50, 100], 'learning_rate': [0.5, 1, 2]}\n",
        "            else:\n",
        "                parameters = {'n_estimators': [50, 100, 150], 'learning_rate': [0.1, 0.5, 1, 2]}\n",
        "            clf = GridSearchCV(ada, parameters, n_jobs=-1, verbose=10)\n",
        "            clf.fit(sig_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'rf':\n",
        "        \n",
        "            rf = RandomForestClassifier(random_state=0)\n",
        "            if reduced:\n",
        "                parameters = {'min_weight_fraction_leaf': [0.1, 0.5],\n",
        "                              'bootstrap': [True, False],\n",
        "                              'max_depth': (2, 5),\n",
        "                              'max_leaf_nodes': (2, 5),\n",
        "                              'n_estimators': (100, 200)}\n",
        "            else:\n",
        "                parameters = {'min_weight_fraction_leaf': [0.01, 0.1, 0.5],\n",
        "                              'bootstrap': [True, False],\n",
        "                              'max_depth': (2, 5, 10),\n",
        "                              'max_leaf_nodes': (2, 5, 10),\n",
        "                              'n_estimators': (100, 200, 300)}\n",
        "            clf = GridSearchCV(rf, parameters, n_jobs=-1, verbose=10)\n",
        "            clf.fit(sig_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    else:\n",
        "        clf = None\n",
        "\n",
        "    return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uClRHj5zq_c"
      },
      "outputs": [],
      "source": [
        "def time_augment(arr):\n",
        "    return np.vstack((arr.T, np.linspace(0, 1, num=arr.shape[0]))).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00OTEPwDB6W7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def shuffle_dataset(X_train, y_train, X_test, y_test, train_size = 0.8):\n",
        "  X_total = np.concatenate((X_train,X_test),axis=0)\n",
        "  y_total = np.concatenate((y_train,y_test),axis=0)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, train_size = train_size)\n",
        "  return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSeP1nJ4MPI_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYaLWYvnFG7N"
      },
      "outputs": [],
      "source": [
        "def sigkernel_train(X_train, y_train, X_test, y_test):\n",
        "    svc_parameters = {'C': np.logspace(0, 4, 5), 'gamma': list(np.logspace(-4, 4, 9)) + ['auto']}\n",
        "    _sigmas = [1e-2, 2.5e-2, 5e-2, 7.5e-2, 1e-1, 1.5e-1, 2e-1, 2.5e-1, 3e-1, 3.5e-1, 4e-1, 4.5e-1, 5e-1, 5.5e-1, 6e-1, 6.5e-1, 7e-1, 7.5e-1, 8e-1, 8.5e-1, 9e-1, 9.5e-1, 1.]\n",
        "    _scales = [5e-2, 1e-1, 5e-1, 1e0]\n",
        "    best_scores = 0\n",
        "      \n",
        "    best_scores = 0\n",
        "    if X_train.shape[1] <= 200 and X_train.shape[2] <= 8: \n",
        "        transforms = tqdm([(True,True), (False,True), (True,False), (False,False)], position=1, leave=False)\n",
        "    else: # do not try lead-lag as dimension is already high\n",
        "        transforms = tqdm([(True,False), (False,False)], position=1, leave=False)\n",
        "\n",
        "    for (at,ll) in transforms:\n",
        "        transforms.set_description(f\"add-time: {at}, lead-lag: {ll}\")\n",
        "        X_train_tl= X_train / X_train.max()\n",
        "\n",
        "\n",
        "        # path-transform\n",
        "        X_train_tl = sigkernel.transform(X_train_tl, at=at, ll=ll, scale=.1)\n",
        "\n",
        "        X_train_tl = torch.tensor(X_train_tl, dtype=torch.float64, device='cuda')\n",
        "                    \n",
        "        # grid search over sigmas\n",
        "  \n",
        "        for sigma in _sigmas:\n",
        "\n",
        "            # define static kernel\n",
        "            static_kernel = sigkernel.RBFKernel(sigma=sigma)\n",
        "\n",
        "            # initialize corresponding signature PDE kernel\n",
        "            signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order=0)\n",
        "\n",
        "            # compute Gram matrix on train data\n",
        "            G_train = signature_kernel.compute_Gram(X_train_tl, X_train_tl, sym=True, max_batch=40).cpu().numpy()\n",
        "\n",
        "            # SVC sklearn estimator\n",
        "            svc = SVC(kernel='precomputed', decision_function_shape='ovo',probability=True)\n",
        "            svc_model = GridSearchCV(estimator=svc, param_grid=svc_parameters, cv=5, n_jobs=-1)\n",
        "            svc_model.fit(G_train, y_train)\n",
        "                        \n",
        "            # empty memory\n",
        "            del G_train\n",
        "            #torch.cuda.empty_cache()\n",
        "            \n",
        "            # store results\n",
        "            if svc_model.best_score_ > best_scores:\n",
        "                best_scores = svc_model.best_score_\n",
        "                trained_models = (at, ll, sigma, svc_model)\n",
        "\n",
        "            sleep(0.5)\n",
        "    return best_scores,trained_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy7fw7BbFRY6"
      },
      "outputs": [],
      "source": [
        "def sigkernel_test(X_train, y_train, X_test, y_test, trained_models): \n",
        "\n",
        "\n",
        "    X_train_tl= X_train / X_train.max()\n",
        "    X_test_tl= X_test / X_test.max()\n",
        "\n",
        "    at, ll, sigma, estimator = trained_models\n",
        "\n",
        "\n",
        "    X_train_tl = torch.tensor(X_train_tl, dtype=torch.float64, device='cuda')\n",
        "    X_test_tl = torch.tensor(X_test_tl, dtype=torch.float64, device='cuda')\n",
        "\n",
        "    # define static kernel\n",
        "    static_kernel = sigkernel.RBFKernel(sigma=sigma)\n",
        "                        \n",
        "    # initialize corresponding signature PDE kernel\n",
        "    signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order=0)\n",
        "                            \n",
        "    # compute Gram matrix on test data\n",
        "    G_test = signature_kernel.compute_Gram(X_test_tl, X_train_tl, sym=False,max_batch=60).cpu().numpy()\n",
        "\n",
        "    y_pred_proba = estimator.predict_proba(G_test)[:, 1]\n",
        "    y_pred = estimator.predict(G_test)\n",
        "                        \n",
        "    # record score\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    train_score = estimator.best_score_\n",
        "    test_score = estimator.score(G_test, y_test)\n",
        "    return accuracy,test_score,train_score\n",
        "\n",
        "    # empty memory\n",
        "    del G_test\n",
        "    #torch.cuda.empty_cache()\n",
        "\n",
        "    sleep(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-XcT8Wszczk"
      },
      "outputs": [],
      "source": [
        "def auto_ml(X_train, y_train, X_test, y_test, method, sig_level, dataset, ts_scale=True, standard_scale=True,\n",
        "            time_aug=False, train_size = 0.8):\n",
        "    accuracy = []\n",
        "    accuracy_test = []\n",
        "    accuracy_train = []\n",
        "    \n",
        "\n",
        "    method_dict = {\"rf\": \"Random Forests\", \"ada\": \"AdaBoost\", \"knn\": \"K Nearest Neighbours\",\n",
        "                   \"svc\": \"Support Vector Machines\", \"lr\": \"Logistic Regression\",\n",
        "                   \"ts_svc\": \"Time Series Support Vector Machines\",\n",
        "                   \"ts_knn\": \"Time Series K Nearest Neighbours\"}\n",
        "\n",
        "\n",
        "    for i in range(5):\n",
        "        X_train, X_test, y_train, y_test = shuffle_dataset(X_train, y_train, X_test, y_test, train_size)\n",
        "        # initialise scalers\n",
        "        ts_scaler = TimeSeriesScalerMinMax()\n",
        "        scaler = StandardScaler()\n",
        "        if dataset.startswith(\"mi\"):\n",
        "            X_train = np.nan_to_num(X_train[:1000])\n",
        "            X_test = np.nan_to_num(X_test[:250])\n",
        "            y_train = np.nan_to_num(y_train[:1000])\n",
        "            y_test = np.nan_to_num(y_test[:250])\n",
        "\n",
        "        if time_aug:\n",
        "            X_train = np.array(list(map(time_augment, X_train)))\n",
        "            X_test = np.array(list(map(time_augment, X_test)))\n",
        "\n",
        "        if ts_scale:\n",
        "            X_train = ts_scaler.fit_transform(X_train)\n",
        "            X_test = ts_scaler.fit_transform(X_test)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            X_train_torch = torch.from_numpy(X_train).cuda()\n",
        "            X_test_torch = torch.from_numpy(X_test).cuda()\n",
        "\n",
        "            sig_train_unscaled = signatory.signature(X_train_torch, sig_level)\n",
        "            sig_train_unscaled = sig_train_unscaled.cpu().numpy()\n",
        "            sig_test_unscaled = signatory.signature(X_test_torch, sig_level)\n",
        "            sig_test_unscaled = sig_test_unscaled.cpu().numpy()\n",
        "\n",
        "        else:\n",
        "            X_train_torch = torch.from_numpy(X_train)\n",
        "            X_test_torch = torch.from_numpy(X_test)\n",
        "\n",
        "            sig_train_unscaled = signatory.signature(X_train_torch, sig_level)\n",
        "            sig_train_unscaled = sig_train_unscaled.numpy()\n",
        "            sig_test_unscaled = signatory.signature(X_test_torch, sig_level)\n",
        "            sig_test_unscaled = sig_test_unscaled.numpy()\n",
        "\n",
        "        if standard_scale:\n",
        "            sig_train = scaler.fit_transform(sig_train_unscaled)\n",
        "            sig_test = scaler.fit_transform(sig_test_unscaled)\n",
        "        else:\n",
        "            sig_train = sig_train_unscaled\n",
        "            sig_test = sig_test_unscaled\n",
        "\n",
        "        # file name formatting\n",
        "        file_name = f\"{dataset}_{method}_{i}\"\n",
        "        if not method.startswith(\"ts\"):\n",
        "            file_name += f\"_sig{sig_level}\"\n",
        "        if ts_scale:\n",
        "            file_name += \"_ts_scale\"\n",
        "        if standard_scale:\n",
        "            file_name += \"_standard_scale\"\n",
        "        if time_aug:\n",
        "            file_name += \"_time_aug\"\n",
        "    \n",
        "        if dataset.startswith(\"mi\"):\n",
        "            clf = ml_method_setup(method, X_train, sig_train, y_train, file_name, reduced=True)\n",
        "        else:\n",
        "            clf = ml_method_setup(method, X_train, sig_train, y_train, file_name)\n",
        "        print(i)\n",
        "\n",
        "        # fit to data\n",
        "        if method.startswith(\"ts\"):\n",
        "            y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "            y_pred = clf.predict(X_test)\n",
        "            cv_score = cross_val_score(clf, X_train, y_train, scoring='roc_auc', n_jobs=-1)\n",
        "            X_test_accuracy = X_test\n",
        "        else:\n",
        "            y_pred_proba = clf.predict_proba(sig_test)[:, 1]\n",
        "            y_pred = clf.predict(sig_test)\n",
        "            cv_score = cross_val_score(clf, sig_train, y_train, scoring='roc_auc', n_jobs=-1)\n",
        "            X_test_accuracy = sig_test\n",
        "\n",
        "        \n",
        "        accuracy.append(accuracy_score(y_test, y_pred))\n",
        "        accuracy_test.append(clf.score(X_test_accuracy, y_test))\n",
        "        \n",
        "\n",
        "    accuracy_final = sum(accuracy) / len(accuracy)\n",
        "    accuracy_test_fianl = sum(accuracy_test) / len(accuracy_test)\n",
        "    \n",
        "\n",
        "    \n",
        "    file_name = f\"{dataset}_{method}\"\n",
        "    if not method.startswith(\"ts\"):\n",
        "        file_name += f\"_sig{sig_level}\"\n",
        "    if ts_scale:\n",
        "        file_name += \"_ts_scale\"\n",
        "    if standard_scale:\n",
        "        file_name += \"_standard_scale\"\n",
        "    if time_aug:\n",
        "        file_name += \"_time_aug\"\n",
        "    score_df = pd.DataFrame(columns=[\"Accuracy\", \"Accuracy_test\"], index=[\"value\"])\n",
        "    score_df[\"Accuracy\"] = accuracy\n",
        "    score_df[\"Accuracy_test\"] = accuracy_test_fianl\n",
        "    score_df.to_csv(f\"/content/drive/My Drive/results/{file_name}.csv\")\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWNTNUV8BIHF"
      },
      "outputs": [],
      "source": [
        "def sigkernel_auto(dataset, train_size = 0.8, ts_scale = True):\n",
        "    X_train, y_train, X_test, y_test = write_alcoholic(dataset)\n",
        "\n",
        "    accuracy = []\n",
        "    accuracy_test = []\n",
        "    accuracy_train = []\n",
        "\n",
        "    svc_parameters = {'C': np.logspace(0, 4, 5), 'gamma': list(np.logspace(-4, 4, 9)) + ['auto']}\n",
        "    _sigmas = [1e-2, 2.5e-2, 5e-2, 7.5e-2, 1e-1, 1.5e-1, 2e-1, 2.5e-1, 3e-1, 3.5e-1, 4e-1, 4.5e-1, 5e-1, 5.5e-1, 6e-1, 6.5e-1, 7e-1, 7.5e-1, 8e-1, 8.5e-1, 9e-1, 9.5e-1, 1.]\n",
        "    _scales = [5e-2, 1e-1, 5e-1, 1e0]\n",
        "\n",
        "    for i in range(5):\n",
        "      X_train, X_test, y_train, y_test = shuffle_dataset(X_train, y_train, X_test, y_test, train_size)\n",
        "      # initialise scalers\n",
        "      ts_scaler = TimeSeriesScalerMinMax()\n",
        "\n",
        "      if ts_scale:\n",
        "          X_train = ts_scaler.fit_transform(X_train)\n",
        "          X_test = ts_scaler.fit_transform(X_test)\n",
        "        \n",
        "      best_scores, trained_models = sigkernel_train(X_train, y_train, X_test, y_test)\n",
        "      accuracy_sk, accuracy_test_sk, accuracy_train_sk = sigkernel_test(X_train, y_train, X_test, y_test, trained_models)\n",
        "      accuracy.append(accuracy_sk)\n",
        "      accuracy_test.append(accuracy_test_sk)\n",
        "      accuracy_train.append(accuracy_train_sk)\n",
        "      print(accuracy_test)\n",
        "    \n",
        "\n",
        "    accuracy_final = sum(accuracy) / len(accuracy)\n",
        "    accuracy_test_fianl = sum(accuracy_test) / len(accuracy_test)\n",
        "    accuracy_train_final = sum(accuracy_train) / len(accuracy_train)\n",
        "    print(accuracy_final)\n",
        "    print(accuracy_test_fianl)\n",
        "    print(accuracy_train_final)\n",
        "    \n",
        "\n",
        "    file_name = f\"{dataset}_sigkernel\"\n",
        "    if ts_scale:\n",
        "        file_name += \"_ts_scale\"\n",
        "    score_df = pd.DataFrame(columns=[\"Accuracy\", \"Accuracy_test\", \"Accuracy_train\"], index=[\"value\"])\n",
        "    score_df[\"Accuracy\"] = accuracy_final\n",
        "    score_df[\"Accuracy_test\"] = accuracy_test_fianl\n",
        "    score_df[\"Accuracy_train\"] = accuracy_train_final\n",
        "    score_df.to_csv(f\"/content/drive/My Drive/results/{file_name}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fDIeu1E4Fvf"
      },
      "outputs": [],
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK3AY0B--vbr"
      },
      "outputs": [],
      "source": [
        "def main(dataset, method, sig_level, ts_scale=True, standard_scale=True, time_aug=False, train_size=0.8):\n",
        "    X_train, y_train, X_test, y_test = read_data(dataset)\n",
        "\n",
        "    auto_ml(X_train, y_train, X_test, y_test, method, sig_level, dataset,\n",
        "            ts_scale=ts_scale, standard_scale=standard_scale, time_aug=time_aug, train_size=train_size)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXAcJ2Pd-2UT",
        "outputId": "ee512cac-7fe4-4def-9d87-cfdd7b68351f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 468/468 [00:17<00:00, 27.12it/s]\n",
            "100%|██████████| 480/480 [00:17<00:00, 27.83it/s]\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "add-time: True, lead-lag: False:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "add-time: True, lead-lag: False:  50%|█████     | 1/2 [05:58<05:58, 358.60s/it]\u001b[A\n",
            "add-time: False, lead-lag: False:  50%|█████     | 1/2 [05:58<05:58, 358.60s/it]\u001b[A"
          ]
        }
      ],
      "source": [
        "\n",
        "for i in data_list[1:2]:\n",
        "    for ts_scale in [True, False]:\n",
        "      sigkernel_auto(i, train_size = 0.8, ts_scale = True)\n",
        "\n",
        "            \n",
        "        \n",
        "                    \n",
        "              "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyOQSMdbV6VcoMU6cFaaanEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}