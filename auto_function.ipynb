{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiaoqiW09/Sigkernel/blob/main/auto_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI0StgSk7GtC",
        "outputId": "fef64135-16cc-4f73-9753-1ffac9f748f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/crispitagorico/sigkernel.git\n",
            "  Cloning https://github.com/crispitagorico/sigkernel.git to /tmp/pip-req-build-qfvu14mi\n",
            "  Running command git clone -q https://github.com/crispitagorico/sigkernel.git /tmp/pip-req-build-qfvu14mi\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (0.29.32)\n",
            "Requirement already satisfied: numba>=0.50 in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (0.56.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (1.12.1+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (1.0.2)\n",
            "Collecting tslearn\n",
            "  Downloading tslearn-0.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sigkernel==0.0.1) (1.7.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.50->sigkernel==0.0.1) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.50->sigkernel==0.0.1) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.50->sigkernel==0.0.1) (4.13.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.7/dist-packages (from numba>=0.50->sigkernel==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sigkernel==0.0.1) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.50->sigkernel==0.0.1) (3.9.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sigkernel==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sigkernel==0.0.1) (1.2.0)\n",
            "Building wheels for collected packages: sigkernel\n",
            "  Building wheel for sigkernel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sigkernel: filename=sigkernel-0.0.1-cp37-cp37m-linux_x86_64.whl size=276393 sha256=b6350227bc183f790e59cc65458e89368081e99062d8d9211a456cad01e50a44\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-481pl3of/wheels/64/28/de/9c6a353b52fc67cb4988ebd7a5b762e476edaa0af110d23493\n",
            "Successfully built sigkernel\n",
            "Installing collected packages: tslearn, sigkernel\n",
            "Successfully installed sigkernel-0.0.1 tslearn-0.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/crispitagorico/sigkernel.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1z4Sn7k_R_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi3k0Yu87KFb",
        "outputId": "11fd0ee7-da43-41f7-c73a-145d037f2294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting signatory\n",
            "  Downloading signatory-1.2.6.1.9.0.tar.gz (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: signatory\n",
            "  Building wheel for signatory (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for signatory: filename=signatory-1.2.6.1.9.0-cp37-cp37m-linux_x86_64.whl size=7553972 sha256=84b15bb57c36f077afffb9d36798864d45e887a30e63ba9facfa54f1a5e8bc1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/ff/e5/ffe613433c810f7f82da6e0c55abd15f4cc04960f8137db53b\n",
            "Successfully built signatory\n",
            "Installing collected packages: signatory\n",
            "Successfully installed signatory-1.2.6.1.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install signatory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEqm0Ak97LrS",
        "outputId": "b40abc8a-f85b-4fe5-dd66-20a7cec733f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mne\n",
            "  Downloading mne-1.2.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.9.24)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-1.2.1\n"
          ]
        }
      ],
      "source": [
        "pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mx8Bj5j7NAC",
        "outputId": "5465dc6b-5854-433a-878f-88fa679e7d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: iisignature in /usr/local/lib/python3.7/dist-packages (0.24)\n",
            "Requirement already satisfied: numpy>1.7 in /usr/local/lib/python3.7/dist-packages (from iisignature) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "pip install iisignature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wOeWo_N7Rnp"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import signatory\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sigkernel\n",
        "import torch \n",
        "import math\n",
        "import pickle\n",
        "from time import sleep\n",
        "import iisignature as sig\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
        "from tslearn.datasets import UCR_UEA_datasets\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance,TimeSeriesScalerMinMax\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tslearn.svm import TimeSeriesSVC\n",
        "from sklearn.svm import SVC\n",
        "from tslearn.datasets import UCR_UEA_datasets\n",
        "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
        "from tslearn.svm import TimeSeriesSVC\n",
        "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "import joblib\n",
        "import h5py\n",
        "import mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh5HDayQ7SMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326b92ac-b9aa-4b3a-8356-7c90aaffbb30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bEc0U9d7VDd"
      },
      "outputs": [],
      "source": [
        "data_list = ['alcoholic_1', 'alcoholic_12', 'alcoholic_21']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiZkvw0q7Wtc"
      },
      "outputs": [],
      "source": [
        "def write_alcoholic(dataset='alcoholic_1'):\n",
        "    # S1: S1 obj - a single object shown;\n",
        "    s1 = 0\n",
        "    # S12: S2 nomatch - object 2 shown in a non-matching condition (S1 differed from S2)\n",
        "    s12 = 0\n",
        "    # S21: S2 match - object 2 shown in a matching condition (S1 was identical to S2),\n",
        "    s21 = 0\n",
        "    # initialise numpy arrays to fill with time series\n",
        "    s1_X_train_unscaled = np.zeros((160, 256, 64))\n",
        "    s1_y_train = np.zeros(160)\n",
        "    s21_X_train_unscaled = np.zeros((159, 256, 64))\n",
        "    s21_y_train = np.zeros(159)\n",
        "    s12_X_train_unscaled = np.zeros((149, 256, 64))\n",
        "    s12_y_train = np.zeros(149)\n",
        "\n",
        "    # assign numerical values to the classes\n",
        "    classifier = {'a': 1, 'c': 0}\n",
        "\n",
        "    # run through every file in the train directory and import it, using information from the\n",
        "    # matching condition column to determine which experiment was being conducted.\n",
        "    # using this, put the data into the corresponding numpy array\n",
        "    filenames_list = os.listdir('/content/drive/My Drive/alcohol train data')\n",
        "\n",
        "    for file_name in tqdm(filenames_list):\n",
        "        temp_df = pd.read_csv('/content/drive/My Drive/alcohol train data/' + file_name)\n",
        "        if temp_df[\"matching condition\"][0] == \"S1 obj\":\n",
        "            s1_X_train_unscaled[s1] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "            s1_y_train[s1] = classifier[temp_df['subject identifier'][0]]\n",
        "            s1 += 1\n",
        "        if temp_df[\"matching condition\"][0] == \"S2 match\":\n",
        "            s21_X_train_unscaled[s21] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "            s21_y_train[s21] = classifier[temp_df['subject identifier'][0]]\n",
        "            s21 += 1\n",
        "        if temp_df[\"matching condition\"][0] == \"S2 nomatch,\":\n",
        "            s12_X_train_unscaled[s12] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "            s12_y_train[s12] = classifier[temp_df['subject identifier'][0]]\n",
        "            s12 += 1\n",
        "\n",
        "    # t1: S1 obj - a single object shown;\n",
        "    t1 = 0\n",
        "    # t12: S2 nomatch - object 2 shown in a non-matching condition (S1 differed from S2)\n",
        "    t12 = 0\n",
        "    # t21: S2 match - object 2 shown in a matching condition (S1 was identical to S2),\n",
        "    t21 = 0\n",
        "    t1_X_test_unscaled = np.zeros((160, 256, 64))\n",
        "    t1_y_test = np.zeros(160)\n",
        "    t21_X_test_unscaled = np.zeros((160, 256, 64))\n",
        "    t21_y_test = np.zeros(160)\n",
        "    t12_X_test_unscaled = np.zeros((160, 256, 64))\n",
        "    t12_y_test = np.zeros(160)\n",
        "\n",
        "    # same as above but for test data\n",
        "    classifier = {'a': 1, 'c': 0}\n",
        "\n",
        "    # list of filenames in the directory\n",
        "    filenames_list = os.listdir('/content/drive/My Drive/UROP alcohol test data')\n",
        "\n",
        "    for file_name in tqdm(filenames_list):\n",
        "        if file_name == \"Test\":\n",
        "            pass\n",
        "        else:\n",
        "            temp_df = pd.read_csv('/content/drive/My Drive/UROP alcohol test data/' + file_name)\n",
        "            if temp_df[\"matching condition\"][0] == \"S1 obj\":\n",
        "                t1_X_test_unscaled[t1] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "                t1_y_test[t1] = classifier[temp_df['subject identifier'][0]]\n",
        "                t1 += 1\n",
        "            if temp_df[\"matching condition\"][0] == \"S2 match\":\n",
        "                t21_X_test_unscaled[t21] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "                t21_y_test[t21] = classifier[temp_df['subject identifier'][0]]\n",
        "                t21 += 1\n",
        "            if temp_df[\"matching condition\"][0] == \"S2 nomatch,\":\n",
        "                t12_X_test_unscaled[t12] = np.transpose(np.array(temp_df[\"sensor value\"]).reshape([64, 256]))\n",
        "                t12_y_test[t12] = classifier[temp_df['subject identifier'][0]]\n",
        "                t12 += 1\n",
        "\n",
        "    if dataset == 'alcoholic_1':\n",
        "        X_train = s1_X_train_unscaled\n",
        "        y_train = s1_y_train\n",
        "        X_test = t1_X_test_unscaled\n",
        "        y_test = t1_y_test\n",
        "    elif dataset == 'alcoholic_12':\n",
        "        X_train = s12_X_train_unscaled\n",
        "        y_train = s12_y_train\n",
        "        X_test = t12_X_test_unscaled\n",
        "        y_test = t12_y_test\n",
        "    elif dataset == 'alcoholic_21':\n",
        "        X_train = s21_X_train_unscaled\n",
        "        y_train = s21_y_train\n",
        "        X_test = t21_X_test_unscaled\n",
        "        y_test = t21_y_test\n",
        "    else:\n",
        "        X_train = None\n",
        "        y_train = None\n",
        "        X_test = None\n",
        "        y_test = None\n",
        "\n",
        "    # f = h5py.File(\"/content/drive/My Drive/data/data.h5\", 'a')\n",
        "    # grp = f.create_group(f\"alcoholic_{subset}\")\n",
        "    # grp.create_dataset(\"X_train\", data=X_train, compression=\"gzip\", compression_opts=7)\n",
        "    # grp.create_dataset(\"y_train\", data=y_train, compression=\"gzip\", compression_opts=7)\n",
        "    # grp.create_dataset(\"X_test\", data=X_test, compression=\"gzip\", compression_opts=7)\n",
        "    # grp.create_dataset(\"y_test\", data=y_test, compression=\"gzip\", compression_opts=7)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4HMSiWd7Yeq"
      },
      "outputs": [],
      "source": [
        "def move_to_hdf():\n",
        "    for i in tqdm(os.listdir(\"/content/drive/My Drive/data\")):\n",
        "        if i.endswith(\"npz\"):\n",
        "            npz = np.load(f\"data/{i}\", allow_pickle=True)\n",
        "            X_train = npz['arr_0']\n",
        "            y_train = npz['arr_1']\n",
        "            X_test = npz['arr_2']\n",
        "            y_test = npz['arr_3']\n",
        "\n",
        "            print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep=\"\\n\")\n",
        "\n",
        "            f = h5py.File(\"/content/drive/My Drive/data/data.h5\", 'a')\n",
        "            grp = f.create_group(i[:-4])\n",
        "            grp.create_dataset(\"X_train\", data=X_train, compression=\"gzip\", compression_opts=7)\n",
        "            grp.create_dataset(\"y_train\", data=y_train, compression=\"gzip\", compression_opts=7)\n",
        "            grp.create_dataset(\"X_test\", data=X_test, compression=\"gzip\", compression_opts=7)\n",
        "            grp.create_dataset(\"y_test\", data=y_test, compression=\"gzip\", compression_opts=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGXig9yx7bJq"
      },
      "outputs": [],
      "source": [
        "def read_data(dataset=\"alcoholic_1\"):\n",
        "    data = h5py.File(\"/content/drive/My Drive/data/data.h5\", 'r')\n",
        "    X_train = data[f\"{dataset}/X_train\"][:]\n",
        "    y_train = data[f\"{dataset}/y_train\"][:]\n",
        "    X_test = data[f\"{dataset}/X_test\"][:]\n",
        "    y_test = data[f\"{dataset}/y_test\"][:]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxuog9U17cN6"
      },
      "outputs": [],
      "source": [
        "def ml_method_setup(method, X_train, sig_train, y_train, file_name, reduced=False):\n",
        "    if method == 'ts_knn':\n",
        "        \n",
        "        \n",
        "            if reduced:\n",
        "                clf = GridSearchCV(\n",
        "                    Pipeline([\n",
        "                        ('knn', KNeighborsTimeSeriesClassifier())\n",
        "                    ]),\n",
        "                    {'knn__n_neighbors': range(3, 30, 6), 'knn__weights': ['uniform', 'distance']},\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0),\n",
        "                    n_jobs=-1,\n",
        "                    verbose=10\n",
        "                )\n",
        "            else:\n",
        "                clf = GridSearchCV(\n",
        "                    Pipeline([\n",
        "                        ('knn', KNeighborsTimeSeriesClassifier())\n",
        "                    ]),\n",
        "                    {'knn__n_neighbors': range(3, 30, 2), 'knn__weights': ['uniform', 'distance']},\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0),\n",
        "                    n_jobs=-1,\n",
        "                    verbose=10\n",
        "                )\n",
        "            clf.fit(X_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'ts_svc':\n",
        "      \n",
        "        \n",
        "            if reduced:\n",
        "                clf = GridSearchCV(\n",
        "                    Pipeline([\n",
        "                        ('svc', TimeSeriesSVC(random_state=0, probability=True))\n",
        "                    ]),\n",
        "                    {'svc__kernel': ['rbf', 'poly'], 'svc__shrinking': [True, False],\n",
        "                     'svc__C': [0.1, 1, 10]},\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0),\n",
        "                    n_jobs=-1,\n",
        "                    verbose=10\n",
        "                )\n",
        "            else:\n",
        "                clf = GridSearchCV(\n",
        "                    Pipeline([\n",
        "                        ('svc', TimeSeriesSVC(random_state=0, probability=True))\n",
        "                    ]),\n",
        "                    {'svc__kernel': ['gak', 'rbf', 'poly'], 'svc__shrinking': [True, False],\n",
        "                     'svc__C': [0.1, 0.2, 0.5, 1, 2, 5, 10]},\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0),\n",
        "                    n_jobs=-1,\n",
        "                    verbose=10\n",
        "                )\n",
        "            clf.fit(X_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'lr':\n",
        "        \n",
        "       \n",
        "            lr = LogisticRegression(random_state=0)\n",
        "            parameters = {'C': [0.1, 0.2, 0.5, 1, 2, 5, 10],\n",
        "                          'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
        "            clf = GridSearchCV(lr, parameters, n_jobs=-1, verbose=10)\n",
        "            clf.fit(sig_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'svc':\n",
        "        \n",
        "            svc = SVC(random_state=0, probability=True)\n",
        "            if reduced:\n",
        "                parameters = {'kernel': ['rbf', 'poly'], 'shrinking': [True, False],\n",
        "                              'C': [0.1, 1, 10]}\n",
        "            else:\n",
        "                parameters = {'kernel': ['rbf', 'poly'], 'shrinking': [True, False],\n",
        "                              'C': [0.1, 0.2, 0.5, 1, 2, 5, 10]}\n",
        "            clf = GridSearchCV(svc, parameters, n_jobs=-1, verbose=10)\n",
        "            clf.fit(sig_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'knn':\n",
        "        \n",
        "            knn = KNeighborsClassifier()\n",
        "            parameters = {'n_neighbors': range(3, 30, 2), 'weights': ['uniform', 'distance']}\n",
        "            clf = GridSearchCV(knn, parameters, n_jobs=-1, verbose=10)\n",
        "            clf.fit(sig_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'ada':\n",
        "        \n",
        "            ada = AdaBoostClassifier(random_state=0)\n",
        "            if reduced:\n",
        "                parameters = {'n_estimators': [50, 100], 'learning_rate': [0.5, 1, 2]}\n",
        "            else:\n",
        "                parameters = {'n_estimators': [50, 100, 150], 'learning_rate': [0.1, 0.5, 1, 2]}\n",
        "            clf = GridSearchCV(ada, parameters, n_jobs=-1, verbose=10)\n",
        "            clf.fit(sig_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    elif method == 'rf':\n",
        "        \n",
        "            rf = RandomForestClassifier(random_state=0)\n",
        "            if reduced:\n",
        "                parameters = {'min_weight_fraction_leaf': [0.1, 0.5],\n",
        "                              'bootstrap': [True, False],\n",
        "                              'max_depth': (2, 5),\n",
        "                              'max_leaf_nodes': (2, 5),\n",
        "                              'n_estimators': (100, 200)}\n",
        "            else:\n",
        "                parameters = {'min_weight_fraction_leaf': [0.01, 0.1, 0.5],\n",
        "                              'bootstrap': [True, False],\n",
        "                              'max_depth': (2, 5, 10),\n",
        "                              'max_leaf_nodes': (2, 5, 10),\n",
        "                              'n_estimators': (100, 200, 300)}\n",
        "            clf = GridSearchCV(rf, parameters, n_jobs=-1, verbose=10)\n",
        "            clf.fit(sig_train, y_train)\n",
        "            joblib.dump(clf.best_estimator_, f'/content/drive/My Drive/models/{file_name}.pkl')\n",
        "\n",
        "    else:\n",
        "        clf = None\n",
        "\n",
        "    return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl8bFRVj7e1F"
      },
      "outputs": [],
      "source": [
        "def time_augment(arr):\n",
        "    return np.vstack((arr.T, np.linspace(0, 1, num=arr.shape[0]))).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EfgCgMC7hIi"
      },
      "outputs": [],
      "source": [
        "def shuffle_dataset(X_train, y_train, X_test, y_test, train_size = 0.8):\n",
        "  X_total = np.concatenate((X_train,X_test),axis=0)\n",
        "  y_total = np.concatenate((y_train,y_test),axis=0)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, train_size = train_size)\n",
        "  return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu-tm7Kn7jgP"
      },
      "outputs": [],
      "source": [
        "def auto_ml(X_train, y_train, X_test, y_test, method, sig_level, dataset, ts_scale=True, standard_scale=True,\n",
        "            time_aug=False, train_size = 0.8):\n",
        "    accuracy = []\n",
        "    accuracy_test = []\n",
        "    accuracy_train = []\n",
        "    \n",
        "\n",
        "    method_dict = {\"rf\": \"Random Forests\", \"ada\": \"AdaBoost\", \"knn\": \"K Nearest Neighbours\",\n",
        "                   \"svc\": \"Support Vector Machines\", \"lr\": \"Logistic Regression\",\n",
        "                   \"ts_svc\": \"Time Series Support Vector Machines\",\n",
        "                   \"ts_knn\": \"Time Series K Nearest Neighbours\"}\n",
        "\n",
        "\n",
        "    for i in range(5):\n",
        "        X_train, X_test, y_train, y_test = shuffle_dataset(X_train, y_train, X_test, y_test, train_size)\n",
        "        # initialise scalers\n",
        "        ts_scaler = TimeSeriesScalerMinMax()\n",
        "        scaler = StandardScaler()\n",
        "        if dataset.startswith(\"mi\"):\n",
        "            X_train = np.nan_to_num(X_train[:1000])\n",
        "            X_test = np.nan_to_num(X_test[:250])\n",
        "            y_train = np.nan_to_num(y_train[:1000])\n",
        "            y_test = np.nan_to_num(y_test[:250])\n",
        "\n",
        "        if time_aug:\n",
        "            X_train = np.array(list(map(time_augment, X_train)))\n",
        "            X_test = np.array(list(map(time_augment, X_test)))\n",
        "\n",
        "        if ts_scale:\n",
        "            X_train = ts_scaler.fit_transform(X_train)\n",
        "            X_test = ts_scaler.fit_transform(X_test)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            X_train_torch = torch.from_numpy(X_train).cuda()\n",
        "            X_test_torch = torch.from_numpy(X_test).cuda()\n",
        "\n",
        "            sig_train_unscaled = signatory.signature(X_train_torch, sig_level)\n",
        "            sig_train_unscaled = sig_train_unscaled.cpu().numpy()\n",
        "            sig_test_unscaled = signatory.signature(X_test_torch, sig_level)\n",
        "            sig_test_unscaled = sig_test_unscaled.cpu().numpy()\n",
        "\n",
        "        else:\n",
        "            X_train_torch = torch.from_numpy(X_train)\n",
        "            X_test_torch = torch.from_numpy(X_test)\n",
        "\n",
        "            sig_train_unscaled = signatory.signature(X_train_torch, sig_level)\n",
        "            sig_train_unscaled = sig_train_unscaled.numpy()\n",
        "            sig_test_unscaled = signatory.signature(X_test_torch, sig_level)\n",
        "            sig_test_unscaled = sig_test_unscaled.numpy()\n",
        "\n",
        "        if standard_scale:\n",
        "            sig_train = scaler.fit_transform(sig_train_unscaled)\n",
        "            sig_test = scaler.fit_transform(sig_test_unscaled)\n",
        "        else:\n",
        "            sig_train = sig_train_unscaled\n",
        "            sig_test = sig_test_unscaled\n",
        "\n",
        "        # file name formatting\n",
        "        file_name = f\"{dataset}_{method}_{i}\"\n",
        "        if not method.startswith(\"ts\"):\n",
        "            file_name += f\"_sig{sig_level}\"\n",
        "        if ts_scale:\n",
        "            file_name += \"_ts_scale\"\n",
        "        if standard_scale:\n",
        "            file_name += \"_standard_scale\"\n",
        "        if time_aug:\n",
        "            file_name += \"_time_aug\"\n",
        "    \n",
        "        \n",
        "        clf = ml_method_setup(method, X_train, sig_train, y_train, file_name, reduced=True)\n",
        "        print(i)\n",
        "\n",
        "        # fit to data\n",
        "        if method.startswith(\"ts\"):\n",
        "            y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "            y_pred = clf.predict(X_test)\n",
        "            cv_score = cross_val_score(clf, X_train, y_train, scoring='roc_auc', n_jobs=-1)\n",
        "            X_test_accuracy = X_test\n",
        "        else:\n",
        "            y_pred_proba = clf.predict_proba(sig_test)[:, 1]\n",
        "            y_pred = clf.predict(sig_test)\n",
        "            cv_score = cross_val_score(clf, sig_train, y_train, scoring='roc_auc', n_jobs=-1)\n",
        "            X_test_accuracy = sig_test\n",
        "\n",
        "        \n",
        "        accuracy.append(accuracy_score(y_test, y_pred))\n",
        "        accuracy_test.append(clf.score(X_test_accuracy, y_test))\n",
        "        \n",
        "\n",
        "    accuracy_final = sum(accuracy) / len(accuracy)\n",
        "    accuracy_test_fianl = sum(accuracy_test) / len(accuracy_test)\n",
        "    \n",
        "\n",
        "    \n",
        "    file_name = f\"{dataset}_{method}\"\n",
        "    if not method.startswith(\"ts\"):\n",
        "        file_name += f\"_sig{sig_level}\"\n",
        "    if ts_scale:\n",
        "        file_name += \"_ts_scale\"\n",
        "    if standard_scale:\n",
        "        file_name += \"_standard_scale\"\n",
        "    if time_aug:\n",
        "        file_name += \"_time_aug\"\n",
        "    score_df = pd.DataFrame(columns=[\"Accuracy\", \"Accuracy_test\"], index=[\"value\"])\n",
        "    score_df[\"Accuracy\"] = accuracy_final\n",
        "    score_df[\"Accuracy_test\"] = accuracy_test_fianl\n",
        "    print(accuracy_final,accuracy_test_fianl)\n",
        "    score_df.to_csv(f\"/content/drive/My Drive/results/{file_name}.csv\")\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUGSIm587lgt"
      },
      "outputs": [],
      "source": [
        "def main(dataset, method, sig_level, ts_scale=True, standard_scale=True, time_aug=False, train_size=0.8):\n",
        "    X_train, y_train, X_test, y_test = write_alcoholic(dataset)\n",
        "\n",
        "    auto_ml(X_train, y_train, X_test, y_test, method, sig_level, dataset,\n",
        "            ts_scale=ts_scale, standard_scale=standard_scale, time_aug=time_aug, train_size=train_size)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N14Rbrme8Y7B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14246a11-a2d5-4c80-d166-452718757cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 3/468 [00:01<04:33,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-da7c178e7566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ts_knn'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_alcoholic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   auto_ml(X_train, y_train, X_test, y_test, method, 2, dataset = i,\n\u001b[1;32m     13\u001b[0m               ts_scale=ts, standard_scale=st, time_aug=ta, train_size=0.8)\n",
            "\u001b[0;32m<ipython-input-9-13fa7e540aa2>\u001b[0m in \u001b[0;36mwrite_alcoholic\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/alcohol train data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"matching condition\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"S1 obj\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0ms1_X_train_unscaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sensor value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ],
      "source": [
        "method_list = [#\"rf\", #\"ada\", \"knn\", \n",
        "               #\"svc\", \"lr\",\n",
        "               \"ts_svc\", #\"ts_knn\"\n",
        "               ]\n",
        "sig = 2\n",
        "ta = True\n",
        "st = False\n",
        "ts = True\n",
        "method = 'ts_knn'\n",
        "for i in data_list[1:3]:\n",
        "  X_train, y_train, X_test, y_test = write_alcoholic(i)\n",
        "  auto_ml(X_train, y_train, X_test, y_test, method, 2, dataset = i,\n",
        "              ts_scale=ts, standard_scale=st, time_aug=ta, train_size=0.8)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqW0CCGEMlG2"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7UftnJOk_uiU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyOnNafInyhQdHjDQnAyFxf2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}